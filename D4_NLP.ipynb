{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4b31a8c",
   "metadata": {},
   "source": [
    "# Text analytics - Natural Language Processing\n",
    "\n",
    "After reviewing this notebook, you will be able to perform basic NLP steps, using mainly the popular spaCy library. \n",
    "\n",
    "NLP steps covered in this tutorial include:\n",
    "- text pre-processing\n",
    "- tokenization & lemmatization \n",
    "- finding keywords & transforming text to ML-usable numeric vectors\n",
    "- working with meaning through word embeddings (semantic vectors)\n",
    "- word tagging for named entities - Named Entity Recognition (NER)\n",
    "\n",
    "------\n",
    "\n",
    "### NLP with spaCy\n",
    "https://spacy.io/ (The version used in this tutorial is spaCy 3.4)\n",
    "\n",
    "First, let's import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72426d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy #spacy is a popular NLP library\n",
    "from spacy import displacy #displacy enables visualisation of selected spacy outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6ebbe9",
   "metadata": {},
   "source": [
    "SpaCy works with pre-trained language models which you have to download locally. For this tutorial, we will use the medium-size English model. For a list of models available, see spaCy documentation: https://spacy.io/usage/models\n",
    "\n",
    "While we're at it, let's get a couple of other packages that will be needed later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e259c574",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_md\n",
    "!pip install spacy_langdetect\n",
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cd6221",
   "metadata": {},
   "source": [
    "Once you have the model downloaded, you have to load it in order to be able to use it. (While you download it once only, you then have to load the model for each session/script.) Note that there is a naming convention to call your loaded model \"nlp\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655147d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54785fa",
   "metadata": {},
   "source": [
    "You can then use the loaded model to process text input. Note that there is a standard way to call your processed document 'doc'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d41ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"John is learning about natural language processing. To learn this in Prague is fun for everyone who loves nature and languages.\")\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe57040",
   "metadata": {},
   "source": [
    "On the first sight, the doc object looks pretty much the same as the text input, but in fact it is already processed and contains many useful linguistic attributes.\n",
    "\n",
    "From spaCy documentation:\n",
    "\n",
    ">When you call nlp on a text, spaCy first tokenizes the text to produce a Doc object. The Doc is then processed in several different steps – this is also referred to as the processing pipeline. The pipeline used by the trained pipelines typically includes a tagger, a lemmatizer, a parser and an entity recognizer. Each pipeline component returns the processed Doc, which is then passed on to the next component.\n",
    "\n",
    "The resulting Doc is split into tokens and has attributes coming from the processing steps (the pipeline can of course be customized to include more or less steps as necessary). You can access the tokens by common slicing methods, and attributes by their name/type (https://spacy.io/api/token). Note that punctuation symbols are also included as individual tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6959b68",
   "metadata": {},
   "source": [
    "Let's inspect the processed object in a bit more detail. We can have a look at each token, its lemma, its part of speech tag (POS), whether it is a stopword, whether it consists of alphabetic characters, and its named entity (NE) type - if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034d6772",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nicer format for the output\n",
    "print(\"{:<15} {:<15} {:<10} {:<10} {:<10} {:<10}\".format(\n",
    "    'Token','Lemma','POS','stop','alpha','NE'))\n",
    "# Iterating over the tokens and accessing some of their attributes.\n",
    "# (Note: attribute .lemma returns a numerical index of each lemma, .lemma_ returns the actual value)\n",
    "for i in doc:\n",
    "    print(\"{:<15} {:<15} {:<10} {:<10} {:<10} {:<10}\".format(\n",
    "        i.text, i.lemma_, i.pos_, i.is_stop, i.is_alpha,i.ent_type_))\n",
    "\n",
    "# accessing individual tokens & sentences\n",
    "\n",
    "print(\"Last 5 tokens: \", doc[-5:])\n",
    "print(\"First sentence: \", list(doc.sents)[0])\n",
    "print(\"Number of tokens: \", len(doc))\n",
    "#doc.sents is a generator, to work with the actual sentences, make it into a list \n",
    "print(\"Number of sentences: \", len(list(doc.sents))) \n",
    "print(\"Number of identified entities: \", len(doc.ents), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e72fee",
   "metadata": {},
   "source": [
    "You can see that the tokens still keep their upper/lower case as well as the exact grammatical form in which they appear in the text, while lemmas have both normalized form and case. Punctuation is also represented as tokens and has its own lemma. For further working with the text, it is better to do some pre-processing so that we keep only the most relevant words in their normalized form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fdc39c",
   "metadata": {},
   "source": [
    "Side note: spaCy pipelines are language dependent. In case you are dealing with documents in unknown or mixed languages (or aim to categorize documents based on their language), you might want to include language detection as a first step of your pipeline. Language detection in general works more reliably on longer examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf65e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports & adding language detection to the pipeline\n",
    "from spacy_langdetect import LanguageDetector\n",
    "from spacy.language import Language\n",
    "def create_lang_detector(nlp, name):\n",
    "    return LanguageDetector()\n",
    "\n",
    "Language.factory(\"language_detector\", func=create_lang_detector)\n",
    "nlp.add_pipe('language_detector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ff6a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"This is an English example.\",\n",
    "        \"Zkusíme i jiné jazyky.\",\n",
    "        \"Es funkzioniert ganz gut.\",\n",
    "        \"What happens if we do mix jazyky dohromady?\",\n",
    "        \"Málo slov\",\n",
    "        \"je\",\n",
    "        \"těžší určit\"]\n",
    "\n",
    "# Explore the detected languages.\n",
    "for text in texts:\n",
    "    doc_detect = nlp(text)\n",
    "    print(text, doc_detect._.language)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf4007",
   "metadata": {},
   "source": [
    "### Text pre-processing\n",
    "\n",
    "Typical pre-processing steps include: removing punctuation, lowercasing, removing stopwords (frequently occuring words that don't significantly contribute to unique meaning of a text/document), and normalization - either stemming or lemmatization. Resulting normalized text can be further used e.g. as input for ML classification.\n",
    "\n",
    "Let's have a look at how a text document differs at different stages of pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b4e381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all tokens in a text\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "# list of all tokens without punctuation and other characters.\n",
    "# Note that here we simply go for tokens that contain alphabetic characters.\n",
    "# You could also be more generic and specify which exact character types you would like to avoid:\n",
    "# tokens_no_punct = [token for token in doc if not token.is_punct | token.is_space]\n",
    "tokens_no_punct = [token for token in doc if token.is_alpha]\n",
    "\n",
    "# list of all tokens without punctuation, all lowercase\n",
    "tokens_no_punct_lower =  [token.text.lower() for token in doc if token.is_alpha]\n",
    "\n",
    "# list of all lowercased tokens without punctuation + removing stopwords\n",
    "tokens_no_punct_lower_no_stopwords = [token.text.lower() for token in doc if token.is_alpha & (token.is_stop == False)]\n",
    "\n",
    "# list of all lemmas without punctuation and stopwords (lemmas have unified case automatically)\n",
    "lemmas = [token.lemma_ for token in doc if token.is_alpha & (token.is_stop == False)]\n",
    "\n",
    "# Let's look at some statistics about are sample text.\n",
    "print(\"Number of tokens: \", len(tokens))\n",
    "print(\"Number of tokens without punctuation: \",len(tokens_no_punct))\n",
    "print(\"Number of tokens without punctuation and stopwords: \", len(tokens_no_punct_lower_no_stopwords))\n",
    "print(\"Number of lemmas without punctuation and stopwords\", len(lemmas))\n",
    "print(\"Number of unique tokens without punctuation and stopwords: \", len(set(tokens_no_punct_lower_no_stopwords)))\n",
    "print(\"Number of unique lemmas without punctuation and stopwords\", len(set(lemmas)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347302c1",
   "metadata": {},
   "source": [
    "You can see that removing stopwords and punctuation significantly shortens the number of items even in such a short text. For processing of longer texts, this can lead to large saving in computing time and resources. Also note that there is a smaller number of unique lemmas than unique tokens, this is because different forms of the same word get united under the same lemma (see the two lists below for a comparison). Also while we did lowercase all the tokens, case is unified automatically for lemmas, lowercasing generic words, but keeping upper case for proper names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd809eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(set(tokens_no_punct_lower_no_stopwords)))\n",
    "print(sorted(set(lemmas)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f7f67f",
   "metadata": {},
   "source": [
    "##### Hands-on #1\n",
    "*Let's try all this out on a Wikipedia article. In this exercise we will demonstrate that using pre-processing and filtering out of stopwords and punctuation is crucial for getting meaningful insights from a text.*\n",
    "\n",
    "*First we get the content of a selected Wikipedia page (simply run the cell bellow).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9979f795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "turing = wikipedia.page(\"Turings test\").content\n",
    "#show first 1000 characters of the text\n",
    "turing[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f85e90",
   "metadata": {},
   "source": [
    "*Your task is to process the text using spaCy's default pipeline. Then create two separate lists:*\n",
    "\n",
    "    - tokens_turing, containing simply all the tokens from the text without any preprocessing\n",
    "    - lemmas_turing, containing all the lemmas from the text, cleaned from punctuation and stop words\n",
    "    \n",
    "*Additional questions:*\n",
    "\n",
    "1. How many sentences are there in the text?\n",
    "2. What is the last sentence?\n",
    "3. What is the number of unique tokens and lemmas in the text?\n",
    "\n",
    "*Hint: You can use set(x) to create a set of unique values from a list x*\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3d04a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code here\n",
    "doc_turing = \n",
    "tokens_turing = \n",
    "lemmas_turing =\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9fdb24",
   "metadata": {},
   "source": [
    "*Finally, we can use your prepared lists to see what are the most frequent tokens vs lemmas of the text. Run the following cell to see that text pre-processing not only can save us computing time by reducing the number of analyzed terms, it also leads to more meaningful text summary.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42b8529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_freq_tokens = Counter(tokens_turing)\n",
    "word_freq_lemmas = Counter(lemmas_turing)\n",
    "print(\"Most frequent tokens: \", word_freq_tokens.most_common(10), \"\\n\")\n",
    "print(\"Most frequent lemmas: \",word_freq_lemmas.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf9ba70",
   "metadata": {},
   "source": [
    "*Now that you pre-processed the text, you can actually use the lemmas to generate a quick summary of the text in form of a popular graphic - the wordcloud. Just run the cell bellow to see it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855aa43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Generate word cloud from our lemmas dataset.\n",
    "# Note that hte wordlcoud library expects input as a string.\n",
    "word_cloud = WordCloud(background_color = 'white').generate(\" \".join(lemmas_turing))\n",
    "# Display the wordcloud\n",
    "plt.imshow(word_cloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21bd576",
   "metadata": {},
   "source": [
    "### Feature extraction from a text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0e660e",
   "metadata": {},
   "source": [
    "Our generated list of lemmas is already just a few lines of code away from having a bag-of-words representation of the whole text. Let's first summarize the preprocessing steps in one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6143b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(input_text):\n",
    "    doc = nlp(input_text)\n",
    "    # lemmatization of significant tokens of text\n",
    "    lemmas_cleaned = [token.lemma_ for token in doc if \n",
    "                                 token.is_alpha & (token.is_stop == False)]\n",
    "    # joining tokens into a string\n",
    "    processed_text = ' '.join(lemmas_cleaned)\n",
    "    return processed_text\n",
    "\n",
    "# sample input\n",
    "texts = [\"We love apples and we love cherries.\",\n",
    "       \"We love apples and we love bananas.\",\n",
    "        \"We love apples and we love pears\"]\n",
    "# pre-process the input\n",
    "preprocessed = [preprocess_text(t) for t in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bfd79a",
   "metadata": {},
   "source": [
    "Now, with the pre-processed input, we can continue to creat the actual Bag-of-Words representation. It will collect all the words form all documents in our processed input and use them as vocabulary, and it will also count the occurences of each vocabulary item in each document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4838552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# initialize the vectorizer\n",
    "bow = CountVectorizer()\n",
    "# transform words into a count vector\n",
    "features_bow = bow.fit_transform(preprocessed)\n",
    "\n",
    "# get the full vocabulary\n",
    "vocab_bow = bow.get_feature_names_out()\n",
    "# saving the word counts as an array (this can then be used as features for ML models)\n",
    "features_array_bow = features_bow.toarray()\n",
    "# inspect the vocab file\n",
    "print(len(vocab_bow))\n",
    "print(vocab_bow)\n",
    "print(features_array_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c7b04e",
   "metadata": {},
   "source": [
    "Let's have a look at the vocabulary sorted by occuerence count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0753069",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cherries_bow = dict(zip(vocab_bow, features_array_bow[0]))\n",
    "features_bananas_bow = dict(zip(vocab_bow, features_array_bow[1]))\n",
    "features_pears_bow = dict(zip(vocab_bow, features_array_bow[2]))\n",
    "\n",
    "print(dict(sorted(features_cherries_bow.items(), key=lambda x: x[1], reverse=True)[:10]))\n",
    "print(dict(sorted(features_bananas_bow.items(), key=lambda x: x[1], reverse=True)[:10]))\n",
    "print(dict(sorted(features_pears_bow.items(), key=lambda x: x[1], reverse=True)[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e90a5e5",
   "metadata": {},
   "source": [
    "Bag of words is one way of vectorizing a text document. It is based solely on the frequency of occurences of the individual terms, which  might conceal some significant information. Therefore, tfidf (term frequency - inversed document frequency) is typically used for vectorization, as it adds information about which words are specific for a particular document. Let's demonstrate this on our sample sentences.\n",
    "\n",
    "We can do the vectorization as we did with BoW, getting to the same vocabulary items. However, the assigned values for each vocabulary item are different now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b96d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "features_tfidf = tfidf.fit_transform(preprocessed)\n",
    "\n",
    "# get the full vocabulary\n",
    "vocab_tfidf = tfidf.get_feature_names_out()\n",
    "# saving the word values as an array (this can then be used as features for ML models)\n",
    "features_array_tfidf = features_tfidf.toarray()\n",
    "# inspect the vocab file\n",
    "print(len(vocab_tfidf))\n",
    "print(vocab_tfidf)\n",
    "print(features_array_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d7312d",
   "metadata": {},
   "source": [
    "Let's inspect the sorted items again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d0cbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cherries_tfidf = dict(zip(vocab_tfidf, features_array_tfidf[0]))\n",
    "features_bananas_tfidf = dict(zip(vocab_tfidf, features_array_tfidf[1]))\n",
    "features_pears_tfidf = dict(zip(vocab_tfidf, features_array_tfidf[2]))\n",
    "\n",
    "print(dict(sorted(features_cherries_tfidf.items(), key=lambda x: x[1], reverse=True)[:10]))\n",
    "print(dict(sorted(features_bananas_tfidf.items(), key=lambda x: x[1], reverse=True)[:10]))\n",
    "print(dict(sorted(features_pears_tfidf.items(), key=lambda x: x[1], reverse=True)[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c4a5da",
   "metadata": {},
   "source": [
    "You can see that while apples and the other types of fruit had the same value in BoW, in tfidf the other fruit types have higer value than apples. This is because apples are mentioned in all three examples, but the other fruit types are specific to each document, thus adding more information for distinguishing between the three."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086c7d4c",
   "metadata": {},
   "source": [
    "##### Hands-on #2\n",
    "\n",
    "*Now let's practice what we have just learned on another example. Load the following Wikipedia articles about Prague, Brno, Bratislava and Budapest and process them. Get both the bag-of-words and tfidf representation of these articles. Looking at the top 5 words for each city and representation types, which words get bigger importance in the tfidf representation?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8a43b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the Wikipedia articles\n",
    "texts = [wikipedia.page(\"prague city czech\").content, \n",
    "         wikipedia.page(\"brno city\").content,\n",
    "         wikipedia.page(\"bratislava city\").content,\n",
    "         wikipedia.page(\"budapest city\").content\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65687fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa259d2e",
   "metadata": {},
   "source": [
    "### Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceba202",
   "metadata": {},
   "source": [
    "The spaCy library also allows you to convert words and entire documents into their vector representation based on their meaning. The library uses a embedding model, which was trained on an extensive corpus.\n",
    "\n",
    "The created vector representations (embeddings) reflect the semantic relationships between words and can be used, for example, to compare words, sentences or whole documents with each other. This is done by calculating the cosine similarity of the compared vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4305eea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = nlp(\"dog\")\n",
    "word2 = nlp(\"cat\")\n",
    "word3 = nlp(\"car\")\n",
    "print(\"word1 to word2: \", word1.similarity(word2))\n",
    "print(\"word1 to word3: \", word1.similarity(word3))\n",
    "print(\"word2 to word3: \", word2.similarity(word3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889d5a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = nlp(\"Kids are buying apples and bananas.\")\n",
    "sent2 = nlp(\"Children are shopping for fruit.\")\n",
    "sent3 = nlp(\"We read a book.\")\n",
    "print(\"sent1 to sent2: \", sent1.similarity(sent2))\n",
    "print(\"sent1 to sent3: \", sent1.similarity(sent3))\n",
    "print(\"sent2 to sent3: \", sent2.similarity(sent3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1de3943",
   "metadata": {},
   "source": [
    "This can be used also for categorization of documents (by calculating similarity between each document and a match phrase/topic), identifying (near) duplicates, or recommendation systems. Note that spaCy embeddings are based on individual words, so embeddings for longer texts will be less accurate, as they can easily get dilluted by included generic words. If you want to compare longer texts with embeddings, it is better to use sentence-encoder-based models (such as Universal Sentence Encoder).\n",
    "\n",
    "##### Hands-on #3\n",
    "*Process the following newspaper headlines with spaCy. Categorize them into articles about medicine/health and articles about sports. Play around with the terms you calculate the similarity on - you can start with simply \"medicine\" and \"sports\", but you can try to extend the terms for example to \"medicine health hospital\" or include individual sports examples, and then see how that influences the result. If using thresholds for setting a category, you can also experiment with different cut-off values.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b26d40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = [\"The new drug slows progression of cognitive decline.\",\n",
    "             \"Cancer is the #2 cause of death in the US.\",\n",
    "             \"The best cookie recipe.\",\n",
    "            \"Basketball players apologize after their fight.\",\n",
    "            \"Rising Covid-19 cases might be a warning sign.\",\n",
    "             \"Player X scored an amazing goal.\",\n",
    "            \"Tennis star reaches the finals.\",\n",
    "             \"Democrats won the election.\",\n",
    "            \"Healthy lifestyle and exercising can lower your risk of early death.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5699b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8534dc3",
   "metadata": {},
   "source": [
    "*As a bonus, you can play around with the following cells and find out what the semantically closest words are to your input words or what happens if you do some basic arithmetic with words.Try to come up with a good triade of words for that!*\n",
    "\n",
    "*We will demonstrate this using a pre-trained GenSim model for a change.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b1471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary imports\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "model = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f068d0f6",
   "metadata": {},
   "source": [
    "*This is how you can get a list of words with most similar vectors*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c95a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712c616c",
   "metadata": {},
   "source": [
    "*And this is how you can calculate with words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd1a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['king', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b48c464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for your own experiments\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9220201",
   "metadata": {},
   "source": [
    "### Named entity recognition (NER)\n",
    "NER is part of the default spaCy pipeline. It uses a pretrained model to identify entities, there are different models for different languages, they might differ in the number of entities they are able to identify and their success rate. For specific entities, other languages or higher success rate, you might need to look for other NER packages/models or train a model of your own.\n",
    "\n",
    "Entities are all grouped under the .ents attribute of your doc, all you can access them directly from a token through .ent_type_ attribute. Each word in an entity also has a flag, whether it is the first word of an entity (B - beginning) or a subsequent word of an entity (I - in) - this can be checked by the .ent_iob_ attribute (non-entities have O - out - as their attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c90cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define text document\n",
    "text = \"\"\"Robin is watching a robin eating an apple on her Apple laptop. \n",
    "          I bought a second jersey in New Jersey yesterday for 20 dollars. \n",
    "          We are sitting at Deloitte office in Vinohrady at 10 am.\"\"\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Check the recognised entities\n",
    "print(\"Identified entities: \", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "# Highlight identified entities directly in text    \n",
    "displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "\n",
    "#Explain a tag\n",
    "print(\"GPE: \", spacy.explain(\"GPE\"))\n",
    "print(\"ORG: \", spacy.explain(\"ORG\"))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Let's have a look at the tags accessed directly from the tokens\n",
    "# last 4 tokens as an example\n",
    "for t in doc[-4:]:\n",
    "    print(t, t.ent_type_, t.ent_iob_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996df237",
   "metadata": {},
   "source": [
    "##### Hands-on #4\n",
    "\n",
    "*Take the input texts and remove any named people by the string \"ANONYMIZED\"*\n",
    "\n",
    "*Hint: You can access the entity type of a token using the ent_type_ attribute*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174bdff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"James Martin Brown was arrested for bribing Jane Newman in California.\n",
    "        Katie had pneumonia in October. Jim, Marie and Bob Adams were accused of assaulting\n",
    "        their cousin David Lim.\"\"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02352d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
