{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After going through these materials, you will be able to use spaCy or other libraries for:\n",
    "\n",
    "- execution of selected NLP use cases,\n",
    "- preprocessing of unstructured texts,\n",
    "- transformation of preprocessed texts into their structured vector representation.\n",
    "\n",
    "First we import the spaCy library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_md==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.3.1/en_core_web_md-2.3.1.tar.gz (50.8 MB)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in c:\\users\\celocaladmin\\appdata\\roaming\\python\\python38\\site-packages (from en_core_web_md==2.3.1) (2.3.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\celocaladmin\\appdata\\roaming\\python\\python38\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (4.47.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (49.2.0.post20200714)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.19.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (0.7.4)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\celocaladmin\\appdata\\roaming\\python\\python38\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\celocaladmin\\appdata\\roaming\\python\\python38\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (0.8.2)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in c:\\users\\celocaladmin\\appdata\\roaming\\python\\python38\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (7.4.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (3.0.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.25.9)\n",
      "Building wheels for collected packages: en-core-web-md\n",
      "  Building wheel for en-core-web-md (setup.py): started\n",
      "  Building wheel for en-core-web-md (setup.py): finished with status 'done'\n",
      "  Created wheel for en-core-web-md: filename=en_core_web_md-2.3.1-py3-none-any.whl size=50916640 sha256=0e2398275478d51b752e9fdb3b08ec91eb1dc92569dd296cd366d885baef461f\n",
      "  Stored in directory: C:\\Users\\CELocalAdmin\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-k6478kjl\\wheels\\b8\\ae\\ae\\c63bae74a3b3e18b2fa35e179387e6cdeb03a1c479ed45c351\n",
      "Successfully built en-core-web-md\n",
      "Installing collected packages: en-core-web-md\n",
      "  Attempting uninstall: en-core-web-md\n",
      "    Found existing installation: en-core-web-md 3.0.0\n",
      "    Uninstalling en-core-web-md-3.0.0:\n",
      "      Successfully uninstalled en-core-web-md-3.0.0\n",
      "Successfully installed en-core-web-md-2.3.1\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy language model\n",
    "\n",
    "The Spacy library is built on trained language models. The language model is the result of training on an annotated corpus of documents in a certain language. \n",
    "\n",
    "Language models differ:\n",
    "- the range of data on which they were trained,\n",
    "- layers/methods that can be used when loading a document.\n",
    "\n",
    "### Load  the model\n",
    "The pre-trained language model is loaded after the library is imported with the load command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.lang.en.English object at 0x0000022A8C83CDF0>\n"
     ]
    }
   ],
   "source": [
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()\n",
    "print(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Processing Pipelines\n",
    "The basic building block of the language model is *Language Processing Pipeline*, that defines the steps applied to unstructured texts within the processing. Default trained pipeline typically include following steps:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each pipeline component returns the processed Doc, which is then\n",
    "passed on to the next component. Spacy pipeline can be modified and additional steps added to it (see section Language detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing and feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of using:\n",
    "- spaCy for text preprocessing,\n",
    "- models for feature extraction.\n",
    "\n",
    "The obtained structured vector representations of the original unstructured documents have the following properties:\n",
    "\n",
    "- appropriately represent the contents of the original unstructured text documents,\n",
    "- are suitable for analysis or to drive machine learning (ML) algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import Binarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(unstructured_text):\n",
    "    print(unstructured_text)\n",
    "    \n",
    "    unstructured_text = nlp(unstructured_text)\n",
    "    \n",
    "    # lemmatization of significant tokens of text\n",
    "    lemmatized_tokenized_text = [token.lemma_ for token in unstructured_text\n",
    "                                 if not token.is_punct | token.is_space | token.is_stop == True]\n",
    "    print(lemmatized_tokenized_text)\n",
    "    \n",
    "    # joining tokens into stream\n",
    "    processed_text = ' '.join(lemmatized_tokenized_text)\n",
    "    print(processed_text)\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction of features/terms from unstructured reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie is very scarier and long\n",
      "['movie', 'scary', 'long']\n",
      "movie scary long\n",
      "This movie is not scary and is slow\n",
      "['movie', 'scary', 'slow']\n",
      "movie scary slow\n",
      "This movie is spooky and good and good\n",
      "['movie', 'spooky', 'good', 'good']\n",
      "movie spooky good good\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['movie scary long', 'movie scary slow', 'movie spooky good good']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewiews = ['This movie is very scarier and long', \n",
    "           'This movie is not scary and is slow', \n",
    "           'This movie is spooky and good and good']\n",
    "\n",
    "preprocessed_rewiews = [preprocess_text(r) for r in rewiews]\n",
    "preprocessed_rewiews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to represent each text document with a fixed structured numeric vector. The procedure of feature extraction depends on the selected model:\n",
    "\n",
    "- Binary vectorizer\n",
    "- Bag of Words (BoW) Model\n",
    "- Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "- pre-trained model BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight in the vector of the given document expresses the fact whether the given term from the dictionary appears in the list of terms of the given document or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good', 'long', 'movie', 'scary', 'slow', 'spooky']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 0)\t1\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "bv = CountVectorizer(binary = True)\n",
    "features = bv.fit_transform(preprocessed_rewiews)\n",
    "\n",
    "# print vocabulary\n",
    "bv.get_feature_names()\n",
    "print(features)\n",
    "print(type(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occurrences of features/terms from vocabulary in the list of features/terms of the given document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'good': 0, 'long': 1, 'movie': 1, 'scary': 1, 'slow': 0, 'spooky': 0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'good': 0, 'long': 0, 'movie': 1, 'scary': 1, 'slow': 1, 'spooky': 0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'good': 1, 'long': 0, 'movie': 1, 'scary': 0, 'slow': 0, 'spooky': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(bv.get_feature_names(), features.toarray()[0]))\n",
    "dict(zip(bv.get_feature_names(), features.toarray()[1]))\n",
    "dict(zip(bv.get_feature_names(), features.toarray()[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structured vector representation of three documents in the feature matrix. This matrix can already be folded as in the input to DM/ML algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 0],\n",
       "       [1, 0, 1, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words (BoW) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight in the vector of a given document expresses the number of occurrences of the given feature/term from vocabulary in the list of features/terms of the given document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good', 'long', 'movie', 'scary', 'slow', 'spooky']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = CountVectorizer()\n",
    "features = bow.fit_transform(preprocessed_rewiews)\n",
    "\n",
    "# print vocabulary\n",
    "bow.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occurrences of features/terms from vocabulary in the list of features/terms of the given document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'good': 0, 'long': 1, 'movie': 1, 'scary': 1, 'slow': 0, 'spooky': 0}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'good': 0, 'long': 0, 'movie': 1, 'scary': 1, 'slow': 1, 'spooky': 0}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'good': 2, 'long': 0, 'movie': 1, 'scary': 0, 'slow': 0, 'spooky': 1}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(bow.get_feature_names(), features.toarray()[0]))\n",
    "dict(zip(bow.get_feature_names(), features.toarray()[1]))\n",
    "dict(zip(bow.get_feature_names(), features.toarray()[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structured vector representation of three documents in the feature matrix. This matrix can already be folded as in the input to DM/ MLalgorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 0],\n",
       "       [1, 0, 1, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "Unlike the BoW model, it represents a more sophisticated approach to creating vector representations of lists of features/terms of the original documents.\n",
    "\n",
    "The weight in the vector of a given document expresses the weight of individual feature/term from vocabulary in the document, in the context of all documents.\n",
    "\n",
    "During the calculation of the vector weights of a given document, this approach does not take into account only the given document (individual list of features/terms), but takes into account the entire document base (all lists of features/terms).\n",
    "\n",
    "Approach intuition:\n",
    "- if the given feature/term occurs in the given document, but also in all others, then the weight of the given feature/term will be negligible in the given document\n",
    "- if the given feature/term occurs in the given document and in no other, then the weight of the given feature/term will be significant in the given document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good', 'long', 'movie', 'scary', 'slow', 'spooky']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "features = tfidf.fit_transform(preprocessed_rewiews)\n",
    "\n",
    "# print vocabulary\n",
    "tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occurrences of features/terms from vocabulary in the list of features/terms of the given document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'good': 0.0,\n",
       " 'long': 0.7203334490549893,\n",
       " 'movie': 0.4254405389711991,\n",
       " 'scary': 0.5478321549274363,\n",
       " 'slow': 0.0,\n",
       " 'spooky': 0.0}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'good': 0.0,\n",
       " 'long': 0.0,\n",
       " 'movie': 0.4254405389711991,\n",
       " 'scary': 0.5478321549274363,\n",
       " 'slow': 0.7203334490549893,\n",
       " 'spooky': 0.0}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'good': 0.864770177579381,\n",
       " 'long': 0.0,\n",
       " 'movie': 0.25537359879528915,\n",
       " 'scary': 0.0,\n",
       " 'slow': 0.0,\n",
       " 'spooky': 0.4323850887896905}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(tfidf.get_feature_names(), features.toarray()[0]))\n",
    "dict(zip(tfidf.get_feature_names(), features.toarray()[1]))\n",
    "dict(zip(tfidf.get_feature_names(), features.toarray()[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structured vector representation of three documents in the feature matrix. This matrix can already be folded as in the input to DM/ ML algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.72033345, 0.42544054, 0.54783215, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.42544054, 0.54783215, 0.72033345,\n",
       "        0.        ],\n",
       "       [0.86477018, 0.        , 0.2553736 , 0.        , 0.        ,\n",
       "        0.43238509]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "For the purpose of feature extraction, we will now use the pre-trained BERT model. It works as a transformer encoder, performing both word and sentence / document embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import of BERT model including tool for tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now tokenize the preprocessed text. It consists in dividing individual texts into tokens, which are then replaced by their identifiers. Finally, the first [CSL] and last [SEP] token is added in the context of each text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[101, 3185, 12459, 2146, 102],\n",
       " [101, 3185, 12459, 4030, 102],\n",
       " [101, 3185, 11867, 14659, 2100, 2204, 2204, 102]]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts = [tokenizer.encode(x, add_special_tokens=True) for x in preprocessed_rewiews]\n",
    "tokenized_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a list of lists, where one list (a list of token identifiers of a given text) represents exactly one document. We are now transforming this output into a matrix form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 8)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[  101,  3185, 12459,  2146,   102,     0,     0,     0],\n",
       "       [  101,  3185, 12459,  4030,   102,     0,     0,     0],\n",
       "       [  101,  3185, 11867, 14659,  2100,  2204,  2204,   102]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = 0\n",
    "for i in tokenized_texts:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "tokenized_texts_matrix = np.array([i + [0]*(max_len-len(i)) for i in tokenized_texts])\n",
    "np.array(tokenized_texts_matrix).shape\n",
    "tokenized_texts_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create an auxiliary matrix. This instructs the BERT model to ignore the artificial fill we created during the generation of the above matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 8)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask_matrix = np.where(tokenized_texts_matrix != 0, 1, 0)\n",
    "attention_mask_matrix.shape\n",
    "attention_mask_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create an input tensor out of the padded token matrix, and send that to BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 8, 768)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.tensor(tokenized_texts_matrix)  \n",
    "attention_mask = torch.tensor(attention_mask_matrix)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "last_hidden_states[0].numpy().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the processing will be returned into last_hidden_states. It takes the form of 768 matrices with three rows (one for each document) and 7 columns (number of tokens in the longest document + added first and last token). Of the given output, we are mainly interested in the output corresponding to the first token [CLS]. It represents vector representations of given preprocessed texts.\n",
    "\n",
    "We obtain vector representations of the given texts by selecting the first column from all matrices. The vector representation of, for example, the first document then corresponds to a vector composed of values located in the first column and the first row across the 768 matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 768)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = last_hidden_states[0][:,0,:].numpy()\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.140289</td>\n",
       "      <td>-0.087130</td>\n",
       "      <td>-0.054506</td>\n",
       "      <td>-0.019885</td>\n",
       "      <td>-0.065086</td>\n",
       "      <td>-0.246025</td>\n",
       "      <td>0.417703</td>\n",
       "      <td>0.253575</td>\n",
       "      <td>-0.464967</td>\n",
       "      <td>0.246297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.312790</td>\n",
       "      <td>-0.215165</td>\n",
       "      <td>0.134715</td>\n",
       "      <td>-0.120480</td>\n",
       "      <td>0.122271</td>\n",
       "      <td>0.041048</td>\n",
       "      <td>0.101864</td>\n",
       "      <td>-0.354336</td>\n",
       "      <td>0.330352</td>\n",
       "      <td>0.151576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.185638</td>\n",
       "      <td>-0.097628</td>\n",
       "      <td>0.030226</td>\n",
       "      <td>0.085130</td>\n",
       "      <td>-0.002963</td>\n",
       "      <td>-0.207856</td>\n",
       "      <td>0.342548</td>\n",
       "      <td>0.278062</td>\n",
       "      <td>-0.376935</td>\n",
       "      <td>0.175713</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241080</td>\n",
       "      <td>-0.187037</td>\n",
       "      <td>0.088140</td>\n",
       "      <td>0.022385</td>\n",
       "      <td>0.105547</td>\n",
       "      <td>-0.016296</td>\n",
       "      <td>0.045817</td>\n",
       "      <td>-0.193567</td>\n",
       "      <td>0.346718</td>\n",
       "      <td>0.090901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.116938</td>\n",
       "      <td>0.305266</td>\n",
       "      <td>-0.030802</td>\n",
       "      <td>0.046325</td>\n",
       "      <td>-0.075578</td>\n",
       "      <td>-0.485095</td>\n",
       "      <td>0.517049</td>\n",
       "      <td>0.243507</td>\n",
       "      <td>-0.266563</td>\n",
       "      <td>-0.004576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079357</td>\n",
       "      <td>-0.266141</td>\n",
       "      <td>0.145711</td>\n",
       "      <td>0.326371</td>\n",
       "      <td>0.162020</td>\n",
       "      <td>0.217918</td>\n",
       "      <td>-0.279784</td>\n",
       "      <td>-0.284029</td>\n",
       "      <td>0.281404</td>\n",
       "      <td>0.125247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.140289 -0.087130 -0.054506 -0.019885 -0.065086 -0.246025  0.417703   \n",
       "1 -0.185638 -0.097628  0.030226  0.085130 -0.002963 -0.207856  0.342548   \n",
       "2 -0.116938  0.305266 -0.030802  0.046325 -0.075578 -0.485095  0.517049   \n",
       "\n",
       "        7         8         9    ...       758       759       760       761  \\\n",
       "0  0.253575 -0.464967  0.246297  ...  0.312790 -0.215165  0.134715 -0.120480   \n",
       "1  0.278062 -0.376935  0.175713  ...  0.241080 -0.187037  0.088140  0.022385   \n",
       "2  0.243507 -0.266563 -0.004576  ...  0.079357 -0.266141  0.145711  0.326371   \n",
       "\n",
       "        762       763       764       765       766       767  \n",
       "0  0.122271  0.041048  0.101864 -0.354336  0.330352  0.151576  \n",
       "1  0.105547 -0.016296  0.045817 -0.193567  0.346718  0.090901  \n",
       "2  0.162020  0.217918 -0.279784 -0.284029  0.281404  0.125247  \n",
       "\n",
       "[3 rows x 768 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(features)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP use cases with Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now show the use of the Spacy library using examples based on the presentation on NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER allows easily identify the key elements in a text, like of:\n",
    "\n",
    "- people,\n",
    "- places,\n",
    "- brands,\n",
    "- monetary values,\n",
    "- and more. \n",
    "\n",
    "Extracting the main entities in a text helps sort unstructured data and detect important information, which is crucial if you have to deal with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: NER form short text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apple', 'ORG'), ('U.K.', 'GPE'), ('$1 billion', 'MONEY')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking at buying \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.K.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " startup for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $1 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('London', 'GPE')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Is always good to eat apple in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    London\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define text document\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# identify and display NEs\n",
    "[(ent.text, ent.label_) for ent in doc.ents]\n",
    "    \n",
    "displacy.render(doc, style=\"ent\")\n",
    "\n",
    "text = \"Is always good to eat apple in London.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# identify and display NEs\n",
    "[(ent.text, ent.label_) for ent in doc.ents]\n",
    "    \n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output, it is clear that in the given document, three named entities are identified and classified:\n",
    "\n",
    "- Apple (organization)\n",
    "- U.K. (country)\n",
    "- $1 billion (money)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: NER from newspaper article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to find out which entities are most mentioned in the article *F.B.I. Agent Peter Strzok, Who Criticized Trump in Texts, Is Fired* published on August 13, 2018 in The New York Times (https://www.nytimes.com/2018/08/13/us/politics/peter-strzok-fired-fbi.html?hp&action=click&pgtype=Homepage&clickSource=story-heading&module=first-column-region&region=top-news&WT.nav=top-news).\n",
    "\n",
    "In the first step, based on the url of the web page using the get method (HTTP method), we obtain an html file containing the analyzed article. Then we extract the text of the article from the html file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "def url_to_string(url):\n",
    "    # get html\n",
    "    res = requests.get(url)\n",
    "    html = res.text\n",
    "    \n",
    "    # extract relevant text from html\n",
    "    soup = BeautifulSoup(html, 'html5lib')\n",
    "    [script.extract() for script in soup([\"script\", \"style\", 'aside'])]\n",
    "    return \" \".join(re.split(r'[\\n\\t]+', soup.get_text()))\n",
    "\n",
    "# get atricle and it's text\n",
    "ny_bb = url_to_string('https://www.nytimes.com/2018/08/13/us/politics/peter-strzok-fired-fbi.html?hp&action=click&pgtype=Homepage&clickSource=story-heading&module=first-column-region&region=top-news&WT.nav=top-news')\n",
    "article = nlp(ny_bb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now display:\n",
    "- total number of recognized NEs\n",
    "- number of NEs by individual categories\n",
    "- most common/most frequent NEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Counter({'ORG': 35,\n",
       "         'PERSON': 87,\n",
       "         'PRODUCT': 1,\n",
       "         'GPE': 11,\n",
       "         'CARDINAL': 4,\n",
       "         'DATE': 23,\n",
       "         'NORP': 2,\n",
       "         'ORDINAL': 1})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('Strzok', 35),\n",
       " ('F.B.I.', 19),\n",
       " ('Trump', 16),\n",
       " ('Russia', 6),\n",
       " ('Clinton', 5),\n",
       " ('Peter Strzok', 4),\n",
       " ('2016', 3),\n",
       " ('Page', 3),\n",
       " ('Horowitz', 3),\n",
       " ('The New York Times', 2)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of recognized NEs\n",
    "len(article.ents)\n",
    "\n",
    "# number of NEs by individual categories\n",
    "Counter([ent.label_ for ent in article.ents])\n",
    "\n",
    "# most common/most frequent NEs\n",
    "Counter([ent.text for ent in article.ents]).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above overview, it is clear that the article informs about certain issues that are, among other things, associated with:\n",
    "- Strzok,\n",
    "- F.B.I.,\n",
    "- Trump,\n",
    "- Russia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we display a random sentence from the article, including the named entities contained in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">     \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    F.B.I.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " Agent \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Peter Strzok\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", Who Criticized \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Trump\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " in Texts, Is Fired - \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The New York Times\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "                                                                                                                  </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = [sen for sen in article.sents]\n",
    "\n",
    "displacy.render(sentences[0], style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'language_detector']\n",
      "['tagger', 'parser', 'ner', 'language_detector']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'language': 'en', 'score': 0.9999973786430825},\n",
       " {'language': 'cs', 'score': 0.9999947869148311},\n",
       " {'language': 'de', 'score': 0.9999972342952657},\n",
       " {'language': 'bg', 'score': 0.9999955246973705}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy_langdetect import LanguageDetector\n",
    "from spacy.language import Language\n",
    "\n",
    "#Language.component(\"language_detector\", func=LanguageDetector())\n",
    "print(nlp.pipe_names)\n",
    "if \"language_detector\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
    "#nlp.pipe_names\n",
    "#nlp.add_pipe('language_detector', last=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "texts = [\"My mom taught me to finish everything on my plate at dinner.\",\n",
    "          \"Cvičení určené na procvičení stavby slov a vět a na určování slovních druhů.\",\n",
    "          \"Tina ist neu in der Stadt und kennt sich noch nicht aus.\",\n",
    "          \"Расцветали яблони и груши.\"]\n",
    "\n",
    "docs = list(nlp.pipe(texts))\n",
    "\n",
    "[text._.language for text in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization\n",
    "\n",
    "Summarization is the task of condensing a piece of text to a shorter version, reducing the size of the initial text while at the same time preserving key informational elements and the meaning of content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: Extractive Text Summarization with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the following procedure is to call up a summary that will consist of the most important sentences of the original text. The importance of a sentence is expressed by the sum of the weights of the keywords that occur in the given sentence. In other words, the sentence is important depending on the importance of the keywords it contains.\n",
    "\n",
    "We first identify the keywords in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "771"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['F.B.I.',\n",
       " 'Agent',\n",
       " 'Peter',\n",
       " 'Strzok',\n",
       " 'Criticized',\n",
       " 'Trump',\n",
       " 'Texts',\n",
       " 'Fired',\n",
       " 'New',\n",
       " 'York']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']\n",
    "keyword = [token.text for token in article \n",
    "           if not token.is_punct | token.is_space | token.is_stop == True \n",
    "           if token.pos_ in pos_tag]\n",
    "\n",
    "len(keyword)\n",
    "keyword[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analyzed text contains 770 keywords.\n",
    "\n",
    "We will now calculate the frequency of occurrence of individual keywords in the text. Then we will display the 5 most frequented of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mr.', 44),\n",
       " ('Strzok', 40),\n",
       " ('F.B.I.', 19),\n",
       " ('Trump', 16),\n",
       " ('investigation', 11)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_word = Counter(keyword)\n",
    "freq_word.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now normalizing these frequencies for better processing. This is accomplished by dividing the frequency of each keyword by the maximum frequency. We get the weights of individual keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('Mr.', 0.022727272727272728),\n",
       " ('Strzok', 0.02066115702479339),\n",
       " ('F.B.I.', 0.00981404958677686),\n",
       " ('Trump', 0.008264462809917356),\n",
       " ('investigation', 0.005681818181818182)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_freq = Counter(keyword).most_common(1)[0][1]\n",
    "max_freq\n",
    "\n",
    "for word in freq_word.keys():  \n",
    "        freq_word[word] = (freq_word[word]/max_freq)\n",
    "\n",
    "freq_word.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this main part of the whole process, we determine the weights of the individual sentences of the text. The weight of a sentence is determined by the weights of individual keywords that occur in the given sentence. Sentence weight expresses the sum of the weights of individual keywords that occur in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.2500000000000004,\n",
       " 0.045454545454545456,\n",
       " 0.045454545454545456,\n",
       " 3.1363636363636362,\n",
       " 0.5]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_strength={}\n",
    "for sent in article.sents:\n",
    "    for word in sent:\n",
    "        if word.text in freq_word.keys(): # is word a keyword?\n",
    "            if sent in sent_strength.keys():\n",
    "                sent_strength[sent]+=freq_word[word.text]\n",
    "            else:\n",
    "                sent_strength[sent]=freq_word[word.text]\n",
    "                \n",
    "list(sent_strength.values())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, nlargest function is used to summarize the string. The nlargest function returns a list containing the top 3 sentences which are stored as *summarized_sentences*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_sentences = nlargest(3, sent_strength, key=sent_strength.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be converted to a string by the following lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Firing Mr. Strzok, however, removes a favorite target of Mr. Trump from the ranks of the F.B.I. and gives Mr. Bowdich and the F.B.I. director, Christopher A. Wray, a chance to move beyond the president’s ire.\\nAdam Goldman and Michael S. SchmidtAug. 13, 2018WASHINGTON — Peter Strzok, the F.B.I. senior counterintelligence agent who disparaged President Trump in inflammatory text messages and helped oversee the Hillary Clinton email and Russia investigations, has been fired for violating bureau policies, Mr. Strzok’s lawyer said Monday.\\nMr. Trump contended that Mr. Strzok targeted the president and accused Mr. Strzok of being “treasonous” and a “disgrace.”'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sentences = [w.text for w in summarized_sentences]\n",
    "summary = '\\n'.join(final_sentences)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word/Document vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spaCy library allows you to convert words and entire documents into their vector representation. The library uses the trained Word2Vec static embedding model, which was trained on an extensive corpus. \n",
    "\n",
    "The created vector representations can be used, for example, to compare words or documents with each other.\n",
    "\n",
    "#### Word vectors and similarity\n",
    "\n",
    "Each existing token has a relationship to the trained model of word vectors, which can be characterized by three attributes:\n",
    "\n",
    "- *has_vector*, if the token has a vector,\n",
    "- *vector_norm*, L2 norm of the token’s vector (the square root of the sum of the values squared),\n",
    "- *OOV*, Out-of-vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>has_vector</th>\n",
       "      <th>vector_norm</th>\n",
       "      <th>is_oov</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>True</td>\n",
       "      <td>6.423194</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>True</td>\n",
       "      <td>4.783220</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salty</th>\n",
       "      <td>True</td>\n",
       "      <td>6.918513</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fries</th>\n",
       "      <td>True</td>\n",
       "      <td>7.299067</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>True</td>\n",
       "      <td>4.657798</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hamburgers</th>\n",
       "      <td>True</td>\n",
       "      <td>7.088755</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>True</td>\n",
       "      <td>4.931635</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           has_vector  vector_norm is_oov\n",
       "I                True     6.423194  False\n",
       "like             True     4.783220  False\n",
       "salty            True     6.918513  False\n",
       "fries            True     7.299067  False\n",
       "and              True     4.657798  False\n",
       "hamburgers       True     7.088755  False\n",
       ".                True     4.931635  False"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"I like salty fries and hamburgers.\")\n",
    "\n",
    "vectors = pd.DataFrame()\n",
    "\n",
    "for token in doc:\n",
    "    vectors.loc[token,\"has_vector\"] = token.has_vector\n",
    "    vectors.loc[token,\"vector_norm\"] = token.vector_norm\n",
    "    vectors.loc[token,\"is_oov\"] = token.is_oov    \n",
    "vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have vectors of individual tokens, then we can proceed to compare these vectors. \n",
    "\n",
    "The *similarity* function is used to calculate the similarity of two vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row0_col0 {\n",
       "            background-color:  #00441b;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row0_col1 {\n",
       "            background-color:  #97d7c7;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row0_col2 {\n",
       "            background-color:  #e5f5f9;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row0_col3 {\n",
       "            background-color:  #eff9fb;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row0_col4 {\n",
       "            background-color:  #dcf2f2;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row0_col5 {\n",
       "            background-color:  #eaf7fa;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row0_col6 {\n",
       "            background-color:  #b4e2d8;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row1_col0 {\n",
       "            background-color:  #78cab1;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row1_col1 {\n",
       "            background-color:  #00441b;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row1_col2 {\n",
       "            background-color:  #d2eeeb;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row1_col3 {\n",
       "            background-color:  #e3f4f7;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row1_col4 {\n",
       "            background-color:  #84cfb9;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row1_col5 {\n",
       "            background-color:  #d1eee9;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row1_col6 {\n",
       "            background-color:  #b0e1d6;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row2_col0 {\n",
       "            background-color:  #f1fafc;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row2_col1 {\n",
       "            background-color:  #f3fafc;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row2_col2 {\n",
       "            background-color:  #00441b;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row2_col3 {\n",
       "            background-color:  #80cdb7;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row2_col4 {\n",
       "            background-color:  #ebf7fa;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row2_col5 {\n",
       "            background-color:  #98d8c9;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row2_col6 {\n",
       "            background-color:  #f7fcfd;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row3_col0 {\n",
       "            background-color:  #f2fafc;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row3_col1 {\n",
       "            background-color:  #f7fcfd;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row3_col2 {\n",
       "            background-color:  #70c6ac;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row3_col3 {\n",
       "            background-color:  #00441b;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row3_col4 {\n",
       "            background-color:  #f5fbfc;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row3_col5 {\n",
       "            background-color:  #127c39;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row3_col6 {\n",
       "            background-color:  #edf8fb;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row4_col0 {\n",
       "            background-color:  #ddf2f3;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row4_col1 {\n",
       "            background-color:  #a7ddd1;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row4_col2 {\n",
       "            background-color:  #ddf2f3;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row4_col3 {\n",
       "            background-color:  #f3fafc;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row4_col4 {\n",
       "            background-color:  #00441b;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row4_col5 {\n",
       "            background-color:  #ebf7fa;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row4_col6 {\n",
       "            background-color:  #9cd9ca;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row5_col0 {\n",
       "            background-color:  #f7fcfd;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row5_col1 {\n",
       "            background-color:  #f2fafc;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row5_col2 {\n",
       "            background-color:  #9ad8ca;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row5_col3 {\n",
       "            background-color:  #16803c;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row5_col4 {\n",
       "            background-color:  #f7fcfd;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row5_col5 {\n",
       "            background-color:  #00441b;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row5_col6 {\n",
       "            background-color:  #f7fcfd;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row6_col0 {\n",
       "            background-color:  #ceede8;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row6_col1 {\n",
       "            background-color:  #e1f4f6;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row6_col2 {\n",
       "            background-color:  #f7fcfd;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row6_col3 {\n",
       "            background-color:  #f7fcfd;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row6_col4 {\n",
       "            background-color:  #b4e2d8;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row6_col5 {\n",
       "            background-color:  #f7fcfd;\n",
       "            color:  #000000;\n",
       "        }    #T_90f00776_c51d_11eb_bbdb_f8e4e3903723row6_col6 {\n",
       "            background-color:  #00441b;\n",
       "            color:  #f1f1f1;\n",
       "        }</style><table id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >I</th>        <th class=\"col_heading level0 col1\" >like</th>        <th class=\"col_heading level0 col2\" >salty</th>        <th class=\"col_heading level0 col3\" >fries</th>        <th class=\"col_heading level0 col4\" >and</th>        <th class=\"col_heading level0 col5\" >hamburgers</th>        <th class=\"col_heading level0 col6\" >.</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723level0_row0\" class=\"row_heading level0 row0\" >I</th>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row0_col1\" class=\"data row0 col1\" >0.555491</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row0_col2\" class=\"data row0 col2\" >0.214086</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row0_col3\" class=\"data row0 col3\" >0.212421</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row0_col4\" class=\"data row0 col4\" >0.316079</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row0_col5\" class=\"data row0 col5\" >0.181652</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row0_col6\" class=\"data row0 col6\" >0.377928</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723level0_row1\" class=\"row_heading level0 row1\" >like</th>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row1_col0\" class=\"data row1 col0\" >0.555491</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row1_col1\" class=\"data row1 col1\" >1.000000</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row1_col2\" class=\"data row1 col2\" >0.300753</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row1_col3\" class=\"data row1 col3\" >0.280542</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row1_col4\" class=\"data row1 col4\" >0.526748</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row1_col5\" class=\"data row1 col5\" >0.306152</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row1_col6\" class=\"data row1 col6\" >0.387020</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723level0_row2\" class=\"row_heading level0 row2\" >salty</th>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row2_col0\" class=\"data row2 col0\" >0.214086</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row2_col1\" class=\"data row2 col1\" >0.300753</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row2_col2\" class=\"data row2 col2\" >1.000000</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row2_col3\" class=\"data row2 col3\" >0.527844</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row2_col4\" class=\"data row2 col4\" >0.249443</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row2_col5\" class=\"data row2 col5\" >0.437844</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row2_col6\" class=\"data row2 col6\" >0.100576</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723level0_row3\" class=\"row_heading level0 row3\" >fries</th>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row3_col0\" class=\"data row3 col0\" >0.212421</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row3_col1\" class=\"data row3 col1\" >0.280542</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row3_col2\" class=\"data row3 col2\" >0.527844</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row3_col3\" class=\"data row3 col3\" >1.000000</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row3_col4\" class=\"data row3 col4\" >0.190629</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row3_col5\" class=\"data row3 col5\" >0.828722</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row3_col6\" class=\"data row3 col6\" >0.165359</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723level0_row4\" class=\"row_heading level0 row4\" >and</th>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row4_col0\" class=\"data row4 col0\" >0.316079</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row4_col1\" class=\"data row4 col1\" >0.526748</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row4_col2\" class=\"data row4 col2\" >0.249443</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row4_col3\" class=\"data row4 col3\" >0.190629</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row4_col4\" class=\"data row4 col4\" >1.000000</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row4_col5\" class=\"data row4 col5\" >0.175625</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row4_col6\" class=\"data row4 col6\" >0.432417</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723level0_row5\" class=\"row_heading level0 row5\" >hamburgers</th>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row5_col0\" class=\"data row5 col0\" >0.181652</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row5_col1\" class=\"data row5 col1\" >0.306152</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row5_col2\" class=\"data row5 col2\" >0.437844</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row5_col3\" class=\"data row5 col3\" >0.828722</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row5_col4\" class=\"data row5 col4\" >0.175625</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row5_col5\" class=\"data row5 col5\" >1.000000</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row5_col6\" class=\"data row5 col6\" >0.099524</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723level0_row6\" class=\"row_heading level0 row6\" >.</th>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row6_col0\" class=\"data row6 col0\" >0.377928</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row6_col1\" class=\"data row6 col1\" >0.387020</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row6_col2\" class=\"data row6 col2\" >0.100576</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row6_col3\" class=\"data row6 col3\" >0.165359</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row6_col4\" class=\"data row6 col4\" >0.432417</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row6_col5\" class=\"data row6 col5\" >0.099524</td>\n",
       "                        <td id=\"T_90f00776_c51d_11eb_bbdb_f8e4e3903723row6_col6\" class=\"data row6 col6\" >1.000000</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x22a7e8c8190>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_map = pd.DataFrame(columns=doc)\n",
    "\n",
    "for token in doc:\n",
    "    for token2 in doc:\n",
    "        similarity_map.loc[token, token2] = token.similarity(token2)\n",
    "\n",
    "similarity_map.apply(pd.to_numeric).style.background_gradient(cmap ='BuGn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document vectors and similarity\n",
    "\n",
    "As with words, you can measure the similarity of entire documents by calling the *similarity* function. The similarity of documents is measured using document vectors, the calculation of which also includes the vectors of individual words.\n",
    "\n",
    "In the following example, we compare a query with two documents, in other words, we calculate the similarity between the query vector and the vectors of both documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_05cd8b2d_c526_11eb_a053_f8e4e3903723row0_col0 {\n",
       "            background-color:  #f5fbfc;\n",
       "            color:  #000000;\n",
       "        }    #T_05cd8b2d_c526_11eb_a053_f8e4e3903723row1_col0 {\n",
       "            background-color:  #f7fcfd;\n",
       "            color:  #000000;\n",
       "        }    #T_05cd8b2d_c526_11eb_a053_f8e4e3903723row2_col0 {\n",
       "            background-color:  #e6f5f9;\n",
       "            color:  #000000;\n",
       "        }    #T_05cd8b2d_c526_11eb_a053_f8e4e3903723row3_col0 {\n",
       "            background-color:  #f0f9fb;\n",
       "            color:  #000000;\n",
       "        }    #T_05cd8b2d_c526_11eb_a053_f8e4e3903723row4_col0 {\n",
       "            background-color:  #00441b;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_05cd8b2d_c526_11eb_a053_f8e4e3903723row5_col0 {\n",
       "            background-color:  #00451c;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_05cd8b2d_c526_11eb_a053_f8e4e3903723row6_col0 {\n",
       "            background-color:  #39a569;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_05cd8b2d_c526_11eb_a053_f8e4e3903723\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Fistfight</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_05cd8b2d_c526_11eb_a053_f8e4e3903723level0_row0\" class=\"row_heading level0 row0\" >This is my sample sentence</th>\n",
       "                        <td id=\"T_05cd8b2d_c526_11eb_a053_f8e4e3903723row0_col0\" class=\"data row0 col0\" >0.155412</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_05cd8b2d_c526_11eb_a053_f8e4e3903723level0_row1\" class=\"row_heading level0 row1\" >This car is beautiful</th>\n",
       "                        <td id=\"T_05cd8b2d_c526_11eb_a053_f8e4e3903723row1_col0\" class=\"data row1 col0\" >0.150065</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_05cd8b2d_c526_11eb_a053_f8e4e3903723level0_row2\" class=\"row_heading level0 row2\" >Movie was not very good</th>\n",
       "                        <td id=\"T_05cd8b2d_c526_11eb_a053_f8e4e3903723row2_col0\" class=\"data row2 col0\" >0.185105</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_05cd8b2d_c526_11eb_a053_f8e4e3903723level0_row3\" class=\"row_heading level0 row3\" >Driving around</th>\n",
       "                        <td id=\"T_05cd8b2d_c526_11eb_a053_f8e4e3903723row3_col0\" class=\"data row3 col0\" >0.164097</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_05cd8b2d_c526_11eb_a053_f8e4e3903723level0_row4\" class=\"row_heading level0 row4\" >Political fights</th>\n",
       "                        <td id=\"T_05cd8b2d_c526_11eb_a053_f8e4e3903723row4_col0\" class=\"data row4 col0\" >0.445665</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_05cd8b2d_c526_11eb_a053_f8e4e3903723level0_row5\" class=\"row_heading level0 row5\" >Boxing</th>\n",
       "                        <td id=\"T_05cd8b2d_c526_11eb_a053_f8e4e3903723row5_col0\" class=\"data row5 col0\" >0.443361</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_05cd8b2d_c526_11eb_a053_f8e4e3903723level0_row6\" class=\"row_heading level0 row6\" >MMA</th>\n",
       "                        <td id=\"T_05cd8b2d_c526_11eb_a053_f8e4e3903723row6_col0\" class=\"data row6 col0\" >0.344525</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x22aad6951c0>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = ['This is my sample sentence',\n",
    "         'This car is beautiful',\n",
    "        \"Movie was not very good\",\n",
    "        \"Driving around\",\n",
    "        \"Political fights\",\n",
    "        \"Boxing\",\n",
    "        \"MMA\"]\n",
    "query = \"Fistfight\" #'Beautiful car'\n",
    "\n",
    "docs = list(nlp.pipe(texts))\n",
    "doc_q = nlp(query)\n",
    "\n",
    "similarity_map = pd.DataFrame()\n",
    "\n",
    "for doc in docs:\n",
    "    similarity_map.loc[doc.text, doc_q.text] = doc_q.similarity(doc)\n",
    "        \n",
    "similarity_map.apply(pd.to_numeric).style.background_gradient(cmap ='BuGn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc_q[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP use cases with other libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SA analyses an incoming text and tells whether the underlying sentiment is:\n",
    "- positive,\n",
    "- negative or\n",
    "- neutral. \n",
    "\n",
    "SA classifies texts according to the sentiment contained in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Rule-based/lexicon-based approach VADER "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VADER (Valence Aware Dictionary and sEntiment Reasoner) method represents a modified dictionary approach (rule-based/lexicon-based approaches) to sentiment analysis. VADER is specifically attuned to sentiments expressed in social media.\n",
    "\n",
    "Characteristics of the VADER method as rule-based/lexicon-based approach:\n",
    "\n",
    "- unlike approaches based on ML methods, VADER does not require any training data,\n",
    "- can very well understand the sentiment of a text containing emoticons, slangs, conjunctions, capital words, punctuations and much more,\n",
    "- works excellent on social media text,\n",
    "- can work with multiple domains.\n",
    "\n",
    "After importing the *SentimentIntensityAnalyzer* method from the vaderSentiment library, we will use the method in the context of four reviews to determine:\n",
    "- polarity score for each sentiment class,\n",
    "- summary compound value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"The food was great! But I didn't like the service.\",\n",
       "  {'neg': 0.219, 'neu': 0.592, 'pos': 0.189, 'compound': -0.1045}),\n",
       " ('I will definitely come again. Great menu.',\n",
       "  {'neg': 0.0, 'neu': 0.424, 'pos': 0.576, 'compound': 0.7783}),\n",
       " ('The atmosphere is nice and the service was helpful.',\n",
       "  {'neg': 0.0, 'neu': 0.556, 'pos': 0.444, 'compound': 0.6808}),\n",
       " (\"Not my style, I don't recommend it.\",\n",
       "  {'neg': 0.26, 'neu': 0.74, 'pos': 0.0, 'compound': -0.2755})]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
    "\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "\n",
    "reviews = [\"The food was great! But I didn't like the service.\",\n",
    "          \"I will definitely come again. Great menu.\",\n",
    "          \"The atmosphere is nice and the service was helpful.\",\n",
    "          \"Not my style, I don't recommend it.\"]\n",
    "\n",
    "# print sentence and it's sentiment's scores\n",
    "[(r, sentiment.polarity_scores(r)) for r in reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculated compound value can be used for classification purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"The food was great! But I didn't like the service.\", 'Negative'),\n",
       " ('I will definitely come again. Great menu.', 'Positive'),\n",
       " ('The atmosphere is nice and the service was helpful.', 'Positive'),\n",
       " (\"Not my style, I don't recommend it.\", 'Negative')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function for determining sentiment based on the compond value calculated by VADER\n",
    "def classify_sentiment(compound_value):\n",
    "    if compound_value >= 0.05 : \n",
    "            return(\"Positive\") \n",
    "\n",
    "    elif compound_value <= - 0.05 : \n",
    "            return(\"Negative\") \n",
    "\n",
    "    else : \n",
    "            return(\"Neutral\")\n",
    "        \n",
    "        \n",
    "[(r, classify_sentiment(sentiment.polarity_scores(r)['compound'])) for r in reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extractive Text Summarization with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SchmidtAug. 13, 2018WASHINGTON — Peter Strzok, the F.B.I. senior counterintelligence agent who disparaged President Trump in inflammatory text messages and helped oversee the Hillary Clinton email and Russia investigations, has been fired for violating bureau policies, Mr. Strzok’s lawyer said Monday.Mr. Trump and his allies seized on the texts — exchanged during the 2016 campaign with a former F.B.I. lawyer, Lisa Page — in assailing the Russia investigation as an illegitimate “witch hunt.” Mr. Strzok, who rose over 20 years at the F.B.I. to become one of its most experienced counterintelligence agents, was a key figure in the early months of the inquiry.Along with writing the texts, Mr. Strzok was accused of sending a highly sensitive search warrant to his personal email account.The F.B.I. had been under immense political pressure by Mr. Trump to dismiss Mr. Strzok, who was removed last summer from the staff of the special counsel, Robert S.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.summarization import summarize\n",
    "\n",
    "extractive_summary = summarize(ny_bb, word_count=100)\n",
    "extractive_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-51-d4e6678da167>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-51-d4e6678da167>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    python -m spacy install \"en_lang_core_sm\"\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -m spacy install \"en_lang_core_sm\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
